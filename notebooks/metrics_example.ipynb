{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, I will show you how to use the `metrics` package in the annotations library. This library includes a wide variety of metrics commonly used to evaluate annotator (dis)agreements, as well as minimal visualisation capabilities. It has two classes: `Metrics` and `Krippendorff`. The reason I separated `Krippendorff` is because it relies on a number of costly functions upon initialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "\n",
    "from disagree import metrics \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a dummy dataset of labels. Remember that current capabilities allow for labels of ascending integers starting at zero, as no labels. So if you have the possible labels `[\"cat\", \"dog\", \"giraffe\", None]`, you will want to convert these to `[0, 1, 2, None]`. \n",
    "\n",
    "The data set in this tutorial will have 15 instances of data, annotated by 3 annotators. The possible labels will be `[0, 1, 2, 3, None]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_annotations = {\"a\": [None, None, None, None, None, 1, 3, 0, 1, 0, 0, 2, 2, None, 2],\n",
    "                    \"b\": [0, None, 1, 0, 2, 2, 3, 2, None, None, None, None, None, None, None],\n",
    "                    \"c\": [None, None, 1, 0, 2, 3, 3, None, 1, 0, 0, 2, 2, None, 3]}\n",
    "df = pd.DataFrame(test_annotations)\n",
    "labels = [0, 1, 2, 3] # Note that you don't need to specify the presence of None labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will explore all of the different metrics available in the `Metrics` class. There are two types here: those that evaluate more than two annotators, and those that evaluate disagreements between two annotators. We will start with the former (this is just the popular Fleiss's kappa metric)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mets = metrics.Metrics(df, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fleiss kappa: -0.29\n"
     ]
    }
   ],
   "source": [
    "fleiss = mets.fleiss_kappa()\n",
    "print(\"Fleiss kappa: {:.2f}\".format(fleiss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5 metrics for the latter type: joint probability, Cohen's kappa, Pearson correlation, Spearman correlation, and Kendall's tau correlation. The latter 3 output a tuple of the correlation and the p-value. \n",
    "\n",
    "Consider an evaluation of how often annotator \"b\" and \"c\" agree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohens = mets.cohens_kappa(ann1=\"b\", ann2=\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint = mets.joint_probability(ann1=\"b\", ann2=\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson = mets.correlation(ann1=\"b\", ann2=\"c\", measure=\"pearson\")\n",
    "spearman = mets.correlation(ann1=\"b\", ann2=\"c\", measure=\"spearman\")\n",
    "kendall = mets.correlation(ann1=\"b\", ann2=\"c\", measure=\"kendall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's kappa: 0.79\n",
      "Joint probability: 0.80\n",
      "Pearson's correlation: (0.9417419115948373, 0.01673155107662241)\n",
      "Spearman's correlation: (0.9210526315789475, 0.026310519685577894)\n",
      "Kendall's correlation: (0.8888888888888888, 0.037356472445581754)\n"
     ]
    }
   ],
   "source": [
    "print(\"Cohen's kappa: {:.2f}\".format(cohens))\n",
    "print(\"Joint probability: {:.2f}\".format(joint))\n",
    "print(\"Pearson's correlation: \" + str(pearson))\n",
    "print(\"Spearman's correlation: \" + str(spearman))\n",
    "print(\"Kendall's correlation: \" + str(kendall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these metrics comparing two annotators, you can visualise the metric in a matrix for all annotators by using the `metric_matrix` method. The only required argument is the function name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.   , 0.33 , 0.732],\n",
       "       [0.33 , 1.   , 0.795],\n",
       "       [0.732, 0.795, 1.   ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mets.metric_matrix(mets.cohens_kappa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Krippendorff's alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Krippendorff's alpha follows a similar logic. This uses the `tqdm` library to output a loading bar as well, because for projects with a very large number of annotators this can take a long time, and has non-linear time complexity. (As an example, for 20,000 instances of data and 5 annotators, this takes about 10 seconds.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/disagree/metrics.py:323: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  self.A = df.as_matrix().T\n",
      "100%|██████████| 4/4 [00:00<00:00, 681.31it/s]\n"
     ]
    }
   ],
   "source": [
    "kripp = metrics.Krippendorff(df, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of different ways to calculate Krippendorff's alpha, depending on the type of data that has been labelled. This is specified using the `data_type` argument seen below. You can use nominal, ordinal, interval, or ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = kripp.alpha(data_type=\"nominal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Krippendorff's alpha: 0.65\n"
     ]
    }
   ],
   "source": [
    "print(\"Krippendorff's alpha: {:.2f}\".format(alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
